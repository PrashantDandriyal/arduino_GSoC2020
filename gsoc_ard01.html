<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>README.md - Grip</title>
  <link rel="icon" href="data:image/x-icon;base64,AAABAAEAEBAAAAEACABoBQAAFgAAACgAAAAQAAAAIAAAAAEACAAAAAAAAAIAAAAAAAAAAAAAAAAAAAAAAAAKCgr/FRUX/xYWGP8YGBr/GRkb/xsbHP8dHR7/Hh4f/x4eIP8fHyD/ICAh/yMjJP8kJCX/JiYo/ygoKv8pKSr/LCwu/y0tLv80NDT/QUFC/0ZGR/9MTE7/UFBR/1JSUv9bW1v/XFxc/2BgYP9hYWP/aGhp/2xsbP9ycnL/d3d3/3l5e/98fHz/f3+A/2dn/P9z3a//cNX8/4CAgP+Xl5f/nZ2e/56en/+fn5//n5+h/6amp/+rq6v/rKys/7CvsP+zs7T/uLi5/7u7u/+8vLz/wMDA/8fHx//Kysr/zc3N/9HR0f/d3d3/3t7e/+Xl5f/q6ur/6+vr/+3t7f/z8/P/9fX1//b29v/39/f/+Pj4//n5+f/6+vr/+/v7//z8/P/9/f3//v7+//////8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/NCYmJiYmJiYmJiYmJiYmNCZKSkpKSkpKSkpKSkpKSiYmSkpKRisdRkMbMklKSkomJkpKSSEiNUhEEQwpSUpKJiZKSi8LHhhAPAwBDDZKSiYmSkoWACA7SEc6HAEaSkomJkpKBxI/SkpKSj4ICkpKJiZKSgUTR0pKSkpGDwZKSiYmSkoTED1KSkpKOQQVSkomJkpKKgowMy4sNycIMUpKJiZKSkEZDgkDAgsNH0VKSiYmSkpKQigUBgkXLUhKSkomJkpKSkpKSkpKSkpKSkpKJiYmJiYmJiYmJiYmJiYmJiYmIyUkNDQ0NDQ0NDQ0NDQmOCYmJiYmJiYmJiYmJiYmOAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=" />
  <link rel="stylesheet" href="//octicons.github.com/components/octicons/octicons/octicons.css" />
  <style>
    /* Page tweaks */
    .preview-page {
      margin-top: 64px;
    }
    /* User-content tweaks */
    .timeline-comment-wrapper > .timeline-comment:after,
    .timeline-comment-wrapper > .timeline-comment:before {
      content: none;
    }
    /* User-content overrides */
    .discussion-timeline.wide {
      width: 920px;
    }
  </style>
</head>
<body>
  <div class="page">
    <div id="preview-page" class="preview-page" data-autorefresh-url="">

    

      <div role="main" class="main-content">
        <div class="container new-discussion-timeline experiment-repo-nav">
          <div class="repository-content">
            <div id="readme" class="readme boxed-group clearfix announce instapaper_body md">
              
                <h3>
                  <span class="octicon octicon-book"></span>
                  README.md - Grip
                </h3>
              
              <article class="markdown-body entry-content" itemprop="text" id="grip-content">
                <h1>
<a id="user-content-gsoc-2020--writing-examples-for-official-libraries" class="anchor" href="#gsoc-2020--writing-examples-for-official-libraries" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>GSoC 2020 | Writing Examples for Official Libraries</h1>
<h2>
<a id="user-content-a-project-proposal-submitted-to-arduino" class="anchor" href="#a-project-proposal-submitted-to-arduino" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>A Project Proposal submitted to Arduino</h2>
<p><strong>Name:</strong> Prashant Dandriyal</p>
<p><strong>GitHub:</strong> <a href="https://github.com/PrashantDandriyal">PrashantDandriyal</a></p>
<p><strong>Email:</strong> <a href="mailto:prashantdandriyal7@gmail.com">prashantdandriyal7@gmail.com</a></p>
<h2>
<a id="user-content-abstract" class="anchor" href="#abstract" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Abstract</h2>
<blockquote>
<p>Short description of your project</p>
</blockquote>
<p>The project proposes to develop examples for the <strong>Arduino_TensorFlowLite</strong> library. The library is supported by several Arduino Boards like the NANO (Classical), NANO BLE Sense and the UNO. All the examples involving the library are demonstrated for the NANO Sense board, but can be deployed on the classical NANO but with externally connected sensors. The examples aim to showcase the practical utility of the on-board sensors of the NANO Sense board and the ease with which almost any lightweight TensorFlow model can be interfaced with it. The examples are explained into two sections: the common <strong>Model Training</strong> part and the device-sensor dependent <strong>Inference</strong> part. We cover all the sensors present and demonstrate the industrial applications. A novel and niche implementation of "On-device training" is also proposed to revisit the need and aspects of TinyML or EmbeddedAI.</p>
<h2>
<a id="user-content-technical-details" class="anchor" href="#technical-details" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Technical Details</h2>
<blockquote>
<p>Long description of the project.</p>
</blockquote>
<p>The <strong>Arduino_TensorFlowLite</strong> library has implicitly been associated with the <em>Arduino Nano 33 BLE Sense</em> board with most efforts from Pete Warden, Daniel Situnayake, Don Coleman, Sandeep Mistry and Dominic Pajak. All the developers have done an excellent job by providing three finely detailed examples like <em>hello_world, magic_wand, Gesture to emoji, person_detection and micro_speech</em>. The board is one of the most prominent work-horses for <em>TinyML</em> that operate under the 1 mW range. These examples have successfully demonstrated the utility and capabilities of the Nano. They use the ported <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech">version</a> of TensorFlowLite and make use of on-board sensors like the digital microphone (MP34DT05), Gesture Sensor for color, ambiance illumination (APDS9960) and IMU (LSM9DS1). The examples provide a clear interface method between the Machine Learning software stack and the hardware sensors. This is exactly what a newcomer seeks; a developer finds it welcoming. I plan to extend this legacy by following the same plan but with newer examples. I will be developing <strong>five examples</strong> for the Nano (classic) and the Nano BLE Sense board. <a href="https://petewarden.com/" rel="nofollow">Pete Warden</a> and Daniel Situnayake have explained the <em>TensorFlow Lite</em> part in their <a href="https://books.google.com/books/about/TinyML.html?id=sB3mxQEACAAJ&amp;source=kp_book_description" rel="nofollow">book</a> on TinyML. They have termed these as experimental and mainly all the tests have been done on the <em>SparkFun</em> Board. In one of the instances, they say:</p>
<blockquote>
<p>Because the default numbers were calibrated for the SparkFun Edge, the Arduino implementation needs its own set of numbers.</p>
</blockquote>
<p>I have taken only a single library owing to its importance and complications that newcomers face when using Machine Learning Models even on local systems. The deployment of these model onto embedded systems is another daunting task to accomplish. I propose to simplify that process by covering all major areas of EdgeAI that the previous contributors may have missed.</p>
<p>The development setup is summarised as:</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Tools</th>
</tr>
</thead>
<tbody>
<tr>
<td>Target Platform(s)</td>
<td>Arduino NANO, Arduino NANO 33 BLE Sense</td>
</tr>
<tr>
<td>IDE</td>
<td>Arduino Web IDE, Arduino Desktop IDE</td>
</tr>
<tr>
<td>Libraries</td>
<td>
<strong>TensorFlow-Arduino Interface</strong> (Arduino_TensorFlowLite, experimental headers, gesture_predictor, etc), <strong>Sensor interface libraries</strong> (ArduinoLSM9DS1, ArduinoSound, ArduinoAPDS9960, ArduinoLPS22HB, ArduinoHTS221), <strong>Pre-installed Board Libraries</strong> (Arduino.h), Miscellaneous</td>
</tr>
</tbody>
</table>
<p>The examples have various common functionalities (like the <code>micro_error_reporter</code> for logging debug information) mostly because all of them use the same "Arduino_TensorFlowLite" library. Hence they share certain dependencies with the pre-existing examples. A common import process seen in all the examples is:</p>
<pre><code>#include &lt;TensorFlowLite.h&gt;
#include &lt;tensorflow/lite/experimental/micro/kernels/all_ops_resolver.h&gt;
#include &lt;tensorflow/lite/experimental/micro/micro_error_reporter.h&gt;
#include &lt;tensorflow/lite/experimental/micro/micro_interpreter.h&gt;
#include &lt;tensorflow/lite/schema/schema_generated.h&gt;
#include &lt;tensorflow/lite/version.h&gt;
</code></pre>
<p>The workflow I will be using is broken into two parts: <strong>1) Model Training</strong> (using TensorFlow Lite API) and <strong>2) Inference</strong> (on Arduino Platform). They are detailed as:</p>
<p><strong>1) Model Training:</strong> TensorFlow 2.0 and TensorFlow Lite APIs are used to train the model using conventional methods. One of the models created in the (later discussed) demo example is created as:</p>
<pre><code>import tensorflow as tf

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(30, activation='relu', dtype='float32')) 
model.add(tf.keras.layers.Dense(15, activation='relu', dtype='float32'))
model.add(tf.keras.layers.Dense(4, activation='softmax',     dtype='float32')) # 4 Classes in output 

</code></pre>
<p>The Trained model is then converted to TensorFlow Lite model.</p>
<blockquote>
<p>Note: The data must be of float32 type</p>
</blockquote>
<p>The model is converted to a header file which is then imported into the <em>main</em> module. It is handled by the "Arduino_TensorFlowLite" library.</p>
<p><strong>2) Inference:</strong> The steps involved are broadly contained in the    <code>Main()</code> module. Each step is implemented through some method provided by the common libraries mentioned above. The steps are:</p>
<ul>
<li>
<p><strong>Model Handler:</strong> The role of this object is to create a handle that allows us to use the model which is basically binary data stored in a byte array which is itself packed in a header file after the training process. It also provides the model description like the schema version.</p>
<pre><code>if (tf_model-&gt;version() != TFLITE_SCHEMA_VERSION) 
{
  error_reporter-&gt;Report(
     "Model provided is schema version %d not equal "
     "to supported version %d.",
     tf_model-&gt;version(), TFLITE_SCHEMA_VERSION);
  return;	// Exit
}
</code></pre>
</li>
<li>
<p><strong>Resolver:</strong> The <code>AllOpsResolver</code> class provides the operations required for inference on microcontrollers and achieves this by providing resolver objects/instances. Usually, all the available operations are pulled in but this is not recommended as this can cause wastage of memory.</p>
<pre><code>// This pulls in all the operation implementations we need.
static tflite::ops::micro::AllOpsResolver resolver;
</code></pre>
</li>
<li>
<p><strong>Interpreter:</strong> The interpreter takes in the model and resolver and some other related parameters and allocates memory for the model's input and outpur tensors (called <em>tensor arena.</em>). After allocation, it is used to access these tensors using pointers.</p>
<pre><code>// Create an interpreter
interpreter = new tflite::MicroInterpreter(model, opsResolver, tensorArena, tensorArenaSize, &amp;errorReporter);

// Allocate memory for the model's input and output tensors
interpreter-&gt;AllocateTensors();

// Get pointers for the model's input and output tensors
inputTensor = interpreter-&gt;input(0);
outputTensor = interpreter-&gt;output(0);
</code></pre>
</li>
</ul>
<p>The above three routines/modules are placed in the <code>setup()</code> sub-routine. The following run indefinitely in the <code>loop()</code> sub-routine, in compliance with Arduino's workflow.</p>
<p>Once the data collected is ready for feeding to the model, i.e., after the required number of samples have been successfully collected and if needed, pre-processed (like normalisation, upsampling/downsampling, etc), the input data is forward propagated and actual inference is made using the follwing modules:</p>
<ul>
<li>
<p><strong>Invoke:</strong> Although a member of the Interpreter object, it performs inference on the input data.</p>
<pre><code>TfLiteStatus invokeStatus = tflInterpreter-&gt;Invoke();
if (invokeStatus != kTfLiteOk) 
    Serial.println("Invoke failed!");
</code></pre>
</li>
</ul>
<p>To avoid hassle of dependency mismanagement, I have kept the new examples with the old ones. These common files are imported <em>as-it-is</em> in the examples, unless stated. The examples are elaborated as follows:</p>
<hr>
<h3>
<a id="user-content-examples" class="anchor" href="#examples" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Examples</h3>
<p><strong>1) On-Device Training (for Function Approximation)</strong>: Demonstration of training on microcontrollers</p>
<p><em>Keywords: Board-Independent, Library Independent</em></p>
<p>This application is simplest yet quite novel and doesn't require any extra sensor module(s). Neural Networks have been proved to be universal function approximators by <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" rel="nofollow">Cybenko</a> and Hornik et al. but with <em>varying degree of approximation</em>. I have implemented some random wiggly continuous functions on the Arduino UNO in bare C and plotted using C++ tools. The interesting part is that this was done using a shallow neural network using a single hidden layer with <code>&lt; 10</code> hidden neurons. The related work can be found in my <a href="https://github.com/PrashantDandriyal/Neural-Networks-In-Cpp/tree/master/Wiggly_Functions">repository</a>. The training was done on the UNO board. The <code>.ino</code> file can be found in the same repository or <a href="https://github.com/PrashantDandriyal/Neural-Networks-In-Cpp/blob/master/arduino_NNv2.0.ino">here</a>. The results were discussed in a paper that I have drafted but is yet to be submitted. The result can be understood with the following plot.</p>
<p><a href="https://github.com/PrashantDandriyal/arduino_GSoC2020/blob/master/DeviceTrain_results.png" target="_blank" rel="noopener noreferrer"><img src="https://github.com/PrashantDandriyal/arduino_GSoC2020/raw/master/DeviceTrain_results.png" alt="Results of Summation function" style="max-width:100%;"></a></p>
<p>It can be clearly observed that model trained on Arduino UNO took 1090 epochs to compete with the accuracy of the one trained on a local system. I plan to port this function approximator to the NANO boards. I will be training a shallow simple Neural Network (approximating a simple summation function), implemented in C with only a single hidden layer. The data is generated using the color sensor. We generate different <em>R, G, B</em> values by keeping different color objects before saving the RBG values. This forms the input vector of the network. The output vector is created by summing up these values (SUM = R+G+B). The model is created within the ino file and trained on this data. Tests are conducted on similar data from the color sensor in a similar fashion. The results can be compared with that of another deeper model. The former will be slighly slow and not as accurate as the deep one. But this is lesson we learn. The second part is implemented using TensorFlow, using the workflow similar to the other examples. The example explains such basic differences in a trivial manner. The on-device training (which takes about 100-200 epochs than the conventional 4GB RAM-based computer systems) is another star to this example. It demonstrates how and why the precision in such microcontrollers drops and how it affects the performance. Almost no microcontrollers demonstrate such a thing, but I would like to do so, which is nothing but an extended version of my previous work.</p>
<p><strong>2) Function Plotter:</strong> Simple regression application based on <em>function approximation</em></p>
<p><em>Keywords: Arduino NANO, Library {Arduino_TensorFlowLite}</em></p>
<p>The example is to be developed in continuation of the previous one, i.e., based on the concept of Function Approximation but implemented as a regression problem using TensorFlow. We train a model on a data generated after creating a random function like _f(p) = 0.2+0.4(p<sup>2</sup>) + 0.3(p Sin(15p) + 0.05 Cos(50p) .</p>
<p><a href="https://github.com/PrashantDandriyal/arduino_GSoC2020/blob/master/wiggly.png" target="_blank" rel="noopener noreferrer"><img src="https://github.com/PrashantDandriyal/arduino_GSoC2020/raw/master/wiggly.png" alt="wiggly func" style="max-width:100%;"></a></p>
<p>The model is trained on the data generated using this function i.e., sets of <em>(x, f(x))</em>. A 3 layer keras model is capable of approximating this function. The results can seen in the below plot, with the BLUE plot as the regressor line predicted by the model. The model is converted and asked to predict on similar real-time data generated within the board. The results are plotted using the terminal. The training part has been demonstrated in the Google Collab <a href="https://github.com/PrashantDandriyal/arduino_GSoC2020/blob/master/Wiggly_function_approximation.ipynb">notebook</a> in the proposal repository. The workflow is as:</p>
<p><code> Create a function in using Python and generate training data -&gt; Create a Keras model -&gt; Train -&gt; Obtain TF Lite model -&gt; Obtain byte array model -&gt; Use model to make prediction on similar data on device -&gt; Plot results on terminal</code></p>
<p><strong>3) Anomaly Detection</strong>: An industrial application</p>
<p><em>Keywords: Arduino NANO, Accelerometer(LSM9DS1), thermometer(HTS221), gyroscope(LSM9DS1), TensorFlow, Library { ArduinoLSM9DS1, ArduinoHTS221, Arduino_TensorFlowLite}</em></p>
<p>The example demonstrates another fine and real-life solution to real problem in industries. The subject has been demonstrated several times and my application is based on the one given by <a href="https://reality.ai/edge-ai-cortex-m7-and-m4/" rel="nofollow">RealityAI</a>. The model is trained on motion, vibration and temperature data, obtained by placing the board on a motor, which is itself mounted on another semi-fixed plank/platform. This is done to emulate an industrial environment where the motor is the equipment being monitored. The model learns to classify the working condition (through analysing the data obtained at that instant) as: <em>Safe</em> or <em>Unsafe</em> for health of the machine/equipment. <em>Unsafe</em> conditions are created by disturbing the motor-board assembly via another motor behind the semi-fixed plank. We demonstrate using only IMU and thermometer data but the example can be used to derive other application that possess other environmental parameters like Pressure. The methodology is:</p>
<p><code>Gather Training Data via IMU and HTS221 -&gt; Manually label data into the 2 classes -&gt; Process data -&gt; Train Model using TensorFlow -&gt; Use the TF_Lite model exported as byte array -&gt; Perform inference on real-time (processed) data</code></p>
<p><strong>4) Activity Tracker</strong>: Smart Wristband-based detection</p>
<p><em>Keywords: Arduino NANO, Accelerometer(LSM9DS1), gyroscope(LSM9DS1), BLE, TensorFlow, Library{ArduinoLSM9DS1, Arduino_TensorFlowLite}</em></p>
<p>The example is based on the fact that the NANO board is fit for the smart wristband applications, owing to its size, power and computation requirements. Another favourable feature is the <em>BLE</em>. It has almost everything needed to develop one such application. The application tracks 4 activities: _walking, jogging standing and siting. These classes are determined at the output of the model inference. This is used to track the duration of each activity. So it becomes clear if the person has been jogging since the last hour. This data is shared to the user's smartphone connected to the device via <em>BLE</em>. This further creates possibilities on the smartphone side. The model is trained on data collected through on-board IMU (Inertial Measurement Unit): accelerometer and gyroscope. This saves us from the need of calibration before performing inference. The process is:</p>
<p><code>Gather Training Data via IMU -&gt; Classify data into classes -&gt; Train Model using TensorFlow -&gt; Use the TF_Lite exported as byte array -&gt; Perform inference on real-time (processed) data</code></p>
<p><strong>5) Sound-based diarization/Classification</strong>: Profiling of sources using Supervised Learning.</p>
<p><em>Keywords: Arduino NANO, Microphone(MP34DT05), TensorFlow, Library{ PDM, ArduinoSound, Arduino_TensorFlowLite}</em></p>
<p>As per the <a href="https://ai.googleblog.com/2018/11/accurate-online-speaker-diarization.html" rel="nofollow">GoogleAI blog</a>, diarization refers to the process of classifying partitions of audio stream with multiple sources/speakers, into different segments associated with each individual source. I propose to develop a related application; instead of breaking down the audio clip, we will be analysing it and displaying the class to which the source(s) are assigned. Just like other examples, we leverage CNN-based Supervised Learning to train a TensorFlow (Lite) model trained on spectrograms of audios generated from different sources like: a human talking, drilling machine at work, nail getting hammered, etc. The model+NANO assembly will be responsible for distinguishing between these sounds when subjected to such an environment, generally an industrial setup. It can be used to create a smart sensor that warns when there is too much of sound generation or rather pollution. It can also determine the number of equipments in work and can warn if there a set of jobs (like humans chatting around when a heavy machine is at work) may be risky for the workers.</p>
<p>The unique part in this example is the part where the feature is generated from the audio, using <em>Fast Fourier Transform</em> (FFT) and converting it into an array. This part is done by the <code>GenerateMicroFeatures()</code> pre-defined in <code>micro_features/micro_features_generator.h</code>. The part that needs deciding, remains the sample size and the sliding window size. Heuristically, a 30ms window is recommended by the authors of the TinyML book. The values obtained in this window is stored as an array using the FFT. The window is shifted by 20ms resulting in a 10ms overlap. Further optimization to reduce data is achieved by averaging these values to reduce the effect of overlapping values. Workflow:</p>
<p><code>Generate audio data using MP34DT05 -&gt; Obtain spectrogram using FFT -&gt; Process it (Perform Down Sampling/Upsampling and normalization/standardisation) -&gt; Train TF model -&gt; Obtain TF Lite model -&gt; Obtain byte-array model -&gt; Perform inference</code></p>
<h3>
<a id="user-content-demonstration-project" class="anchor" href="#demonstration-project" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Demonstration Project:</h3>
<p>To shed more light on the project and the proposed methodology, I have created a prototype project found on the GitHub with name <a href="https://github.com/PrashantDandriyal/arduino_GSoC2020/tree/master/activity_tracker">Activity Tracker</a>. The entire worflow has been shown. The model has been trained on the <a href="https://github.com/mmalekzadeh/motion-sense/tree/master/data">MotionSense</a> dataset, using Google Collaboratory. The process can be seen in the iPython Notebook I saved in the project repository. The notebook can be found <a href="https://github.com/PrashantDandriyal/arduino_GSoC2020/blob/master/activity_tracker_motionSense_gsoc20.ipynb">here</a>. The trained model is then converted to TensorFlow Lite format and finally to a byte array, in the form of a header file. The next part of performing inference on the actual device has been shown in the corresponding <code>.ino</code> file of the project. The model has been tested and the performance can be seen at the end of the <code>ipynb</code> file. Due to inavailability of the NANO board, the complete assembly could not be tested in real-time, nor could the actual IMU data be obtained, but the dataset does well to emulate the project.</p>
<p><em>Note: Another duplicate of the project is placed in the examples section of TensorFlow, to avoid the dependencies issues. This can be found at my <a href="https://github.com/PrashantDandriyal/tensorflow/tree/master/tensorflow/lite/micro/examples/activity_tracker">forked version</a> of TensorFlow</em></p>
<hr>
<h2>
<a id="user-content-schedule-of-deliverables" class="anchor" href="#schedule-of-deliverables" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Schedule of Deliverables</h2>
<blockquote>
<p>Here should come a list of your milestones</p>
</blockquote>
<h3>
<a id="user-content-community-bonding-period-may-4-2020---june-1-2020" class="anchor" href="#community-bonding-period-may-4-2020---june-1-2020" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><strong>Community Bonding Period (May 4, 2020 - June 1, 2020)</strong>
</h3>
<ul>
<li>Discuss approach and weekly targets for Phase 1 with mentors.</li>
<li>Setup working environment for development.</li>
<li>Study about the <code>Arduino_TensorFlowLite</code> and its backend on TensorFlow repository.</li>
<li>Document the dependency network for better understanding.</li>
<li>Obtain the required hardware tools.</li>
<li>Decide mode of communication with mentors and setup channels.</li>
</ul>
<h3>
<a id="user-content-phase-1-june-1-2020---july-3-2020" class="anchor" href="#phase-1-june-1-2020---july-3-2020" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><strong>Phase 1 (June 1, 2020 - July 3, 2020)</strong>
</h3>
<ul>
<li>
<p>Deliverable 1 : <strong>Example 1 (On-Device Training)</strong></p>
<ul>
<li>Generate fake training data using the Summation function (<code>SUM =f(R,B,G)</code> ). (locally)</li>
<li>Train model on local system (locally)</li>
<li>Port training process into the <code>void setup()</code> and <code>void loop()</code> interface of Arduino.</li>
<li>Collect data on-device using the color sensor (APDS9960)</li>
<li>Perform training and display the performance on console.</li>
<li>Test model on real-time data</li>
<li>Compare performance and document it</li>
</ul>
</li>
<li>
<p>Deliverable 2 : <strong>Example 2 (Function Plotter)</strong></p>
<ul>
<li>Obtain fake training data using a random (wiggly) function using Python.</li>
<li>Create and Train a simple neural network using TensorFlow/Keras API.</li>
<li>Convert TF model -&gt; TF Lite model -&gt; <code>model.h</code>
</li>
<li>Create the <code>.ino</code> file after importing the model file.</li>
<li>Generate real-time data (<em>x</em>) and use model to predict (<em>f(x)</em>).</li>
<li>Plot the predicted vs actual result.</li>
<li>Test the system, document issues.</li>
</ul>
</li>
</ul>
<h3>
<a id="user-content-phase-2-july-4-2020---july-27-2020" class="anchor" href="#phase-2-july-4-2020---july-27-2020" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><strong>Phase 2 (July 4, 2020 - July 27, 2020)</strong>
</h3>
<ul>
<li>
<p>Deliverable 3 : <strong>Example 3 (Anomaly Detection)</strong></p>
<ul>
<li>Design and make the motor-plank assembly.</li>
<li>Use the on-board sensor mounted on the assembly to obtain training data. Generate different conditions using the assembly to procure adequate data for both the classes.</li>
<li>Process data (sample it and normalize/standardise).</li>
<li>Train model on data and obtain <code>model.h</code> file as done previously.</li>
<li>Create .ino file. Design data pipeline to be used in it.</li>
<li>Add warning feature for <code>unsafe</code> class (when detected)</li>
<li>Test the system, document issues.</li>
</ul>
</li>
<li>
<p>Deliverable 4 : <strong>Example 4 (Activity Tracker)</strong></p>
<ul>
<li>Obtain training data using the IMU for 3-4 classes indicating motion/no-motion conditions.</li>
<li>Process data and train data using TensorFlow and Python.</li>
<li>Convert model to header format (as done before)</li>
<li>Deploy model to .ino file. Add data handling part to create similar data (as used for training).</li>
<li>Test the system in real-time.</li>
<li>Add BLE feature by connecting to and sending data to another BLE-powered device (smartphone).</li>
<li>Test and debug. Document the observations.</li>
</ul>
</li>
</ul>
<h3>
<a id="user-content-phase-3-july-28-2020---august-23-2020" class="anchor" href="#phase-3-july-28-2020---august-23-2020" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><strong>Phase 3 (July 28, 2020 - August 23, 2020)</strong>
</h3>
<ul>
<li>
<p>Deliverable 5 : <strong>Example 5 (Sound-based diarization/Classification)</strong></p>
<ul>
<li>Obtain training data using on-board microphone.</li>
<li>Process it (Downsample and average values) using FFT to get a numeric array.</li>
<li>Train and convert model to header format.</li>
<li>Import model into .ino file and create data intake pipeline for the audio data. Process it to suit the model.</li>
<li>Test the system in real-time</li>
<li>Debug errors and document.</li>
</ul>
</li>
<li>
<p>Deliverable 6: Project Publishing</p>
<ul>
<li>Create project related to the examples and publish on the Arduino project Hub.</li>
<li>Test Mergeability of code with the actual (Arduino_TensorFlowLite) library.</li>
<li>Provide tutorial to use the library.</li>
<li>Specify the future scope.</li>
</ul>
</li>
</ul>
<h3>
<a id="user-content-final-week-august-24-2020---august-31-2020" class="anchor" href="#final-week-august-24-2020---august-31-2020" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><strong>Final Week (August 24, 2020 - August 31, 2020)</strong>
</h3>
<ul>
<li>Deliverable 7: <strong>Final Project Report</strong>
<ul>
<li>Discuss progress with mentors and seek suggestions on final report.</li>
<li>Get feedback on project.</li>
<li>Make suggested edits.</li>
<li>Prepare final report and get it reviewed by mentors.</li>
</ul>
</li>
</ul>
<hr>
<h2>
<a id="user-content-development-experience" class="anchor" href="#development-experience" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Development Experience</h2>
<blockquote>
<p>Do you have code on GitHub? Can you show previous contributions to other projects?</p>
</blockquote>
<p>I could not find a official repository for the library but have studied it well. Hence, I could not make contributions to it. Besides that, I have been a consistent contributor to the open-source community, the submissions to which can be found in contributions section of my <a href="https://github.com/PrashantDandriyal">GitHub</a> profile. To state them in chronological order:</p>
<ul>
<li>
<p>TensorFlow: The Pull Request was related to issue #35072 opened by Pete Warden (co-founder of TensorFlow Lite and TinyML). It was simply an explanation to "How int8 input and output quantization conversion works in TensorFlow Lite". The Pull has been approved and is ready to pull.</p>
</li>
<li>
<p>Speech-VINO (Intel's OpenVINO toolkit): The page is meant for contributions to Intel's OpenVINO-toolkit-based projects. These projects incoporate sound applications. I have shared my project (in process of the "Introduction to Intel's OpenVINO toolkit" course, in which I am a scholar)</p>
</li>
<li>
<p>Arduino:</p>
<ul>
<li>My first contribution to Arduino has been the modificatios to its "Ciao" <a href="https://github.com/arduino-libraries/Ciao">library</a>. I have updated the <code>readme.md</code> in official format. The Pull Request has been reviewed but merging is not yet done as the responsible maintainers have not gone through it.</li>
<li>Next, I created an issues on Arduino_JSON <a href="https://github.com/arduino-libraries/Arduino_JSON/issues/6">#6</a>
</li>
<li>Participated in discussions on issues <a href="https://github.com/arduino/arduino-cli/issues/566">#566</a> and <a href="https://github.com/arduino/arduino-cli/issues/549">#549</a> in Arduino-cli.</li>
</ul>
</li>
<li>
<p>Miscellaneous: Other Open-Source contributions include the <a href="https://github.com/alihussainia/OpenDevLibrary">OpenVINO</a> project. I have been an active contributor and now, a maintainer to the project. The project aims at creating a hassle-free portable version of the Intel OpenVINO (Open Visual inference and Neural Optimization) toolkit. Till now, the project has had successful runs and no issues.</p>
</li>
</ul>
<blockquote>
<p>Did you do other code related projects or university courses? Do you have experience with Arduino?</p>
</blockquote>
<p>In the University, I am enrolled in the Bachelor in Technology (B.Tech) course with specialization in Electronics and Communications Engineering. I have studied Microcontroller like the 8051 and Embedded Systems as credit courses. In my current semester, I have Neural Networks as a credit subject although I am already working on the topic in relation to "On-device learning for low-computation-capable devices".
My initial experience with microcontrollers began with Arduino; some of the initial projects were related to "Home Automation" and bootloading the ATmega328p manually. I have used the board like the UNO, BluePill (by STM) and several 32 bit microcontroller-based boards by Texas Instruments.</p>
<h2>
<a id="user-content-other-experiences" class="anchor" href="#other-experiences" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Other Experiences</h2>
<p>I am familiar with microcontrollers (both 8 bit &amp; 32 bit) since my sophomore year. I have been a quarter-finalist (top 4% among &gt;10,000 participants) in the India Innovation Challenge and Design Contest (IICDC-2018) during which our team was provided with Texas Instruments tools like the CC26X2R1 and TIVA LAUNCHPAD (EK-TM4C123GXL) EVM. My team's participation in IICDC, 2018 was the first step towards ML. I tried learning more about it by connecting to some of the prominent members of the community (like Evgeni Gousev, the co-founder of TinyML) and followed their guest lectures delivered at Harvard.</p>
<p>I completed the <em>Introduction to TensorFlow</em> course on Udacity and Andrew Ng's course on Coursera and participated in two of the ML competition on HackerEarth. I have been studying Machine Learning for about an year now. Mostly, I have used TensorFlow-Keras as the primary API. Currently I am enrolled into a scholarship programme offered by Udacity in collaboration with Intel, on OpenVINO toolkit, which is another method of implementing edgeAI. Regarding the languages, I have good experience of C, C++ and Python, all of which are he primary languages needed for this project. I have used C++ for several coding competitions held by Google.</p>
<h2>
<a id="user-content-why-this-project" class="anchor" href="#why-this-project" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Why this project?</h2>
<blockquote>
<p>Why you want to do this project ?</p>
</blockquote>
<p>The aim for the selection of this project is based on my interests and experience.
I have been learning and involved with Machine Learning for more than eight months now. Past that I was involved with Embedded Systems (as described above). As I was learning ML, I had a peculiar urge to find a bridge between both my interests. I stumbled upon the <a href="https://www.linkedin.com/groups/13694488?lipi=urn%3Ali%3Apage%3Ad_flagship3_feed%3BN5JXEs0NS4mp083miQjE2A%3D%3D&amp;licu=urn%3Ali%3Acontrol%3Ad_flagship3_feed-feed_list_group" rel="nofollow">TinyML</a>group on LinkedIn and knew it was the exact thing I was looking for. Since then, I have been diving deeper into the field of <strong>EdgeAI</strong> and <strong>TinyML</strong>. I have been planning to submit an official library to Arduino for implementing simple (shallow) Neural Networks on classic Arduino boards like the Uno. But then, GSoC seemed to coincide with it and I took it as an opportunity to continue my initiative but with mentors! The projects I wished to take on were the <em>Writing Examples for Official Libraries</em> and <em>Examples and Tools for Portenta Board</em>. I had some clarifications with the mentor on GitHub and he suggested that I go for this project instead of porting TensorFlow Lite for the Portenta, which I proposed initially.</p>
<p>I believe that the completion of this project will be a contribution to the TinyML and open source Arduino and TensorFlow communities. Both Arduino as well as TensorFlow have been the first choice for beginners of Embedded systems and Machine Learning. The project has the potential of strengthening that tie. The excellent Arduino_TensorFlowLite library deserves more support. This project will encourage other developers to realize the capabilities of the Arduino Boards and port it for other classic boards.</p>
<h2>
<a id="user-content-do-you-have-any-other-commitments-during-the-gsoc-period" class="anchor" href="#do-you-have-any-other-commitments-during-the-gsoc-period" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Do you have any other commitments during the GSoC period?</h2>
<blockquote>
<p>Provide dates, such as holidays, when you will not be available.</p>
</blockquote>
<p>I will be unable on the some of the national gazetted holidays that are mentioned below. I also have my last semester exams in this period.</p>
<table>
<thead>
<tr>
<th>Holiday</th>
<th>Date in 2020</th>
</tr>
</thead>
<tbody>
<tr>
<td>Good Friday</td>
<td>10 April</td>
</tr>
<tr>
<td>Buddha Purnima</td>
<td>7 May</td>
</tr>
<tr>
<td>Ramzan Id</td>
<td>25 May</td>
</tr>
<tr>
<td>Semester Exams</td>
<td>Somewhere in June (not disclosed yet)</td>
</tr>
<tr>
<td>Raksha Bandhan</td>
<td>3 August</td>
</tr>
<tr>
<td>Janmashtmi</td>
<td>11 August</td>
</tr>
<tr>
<td>Indian Independence Day</td>
<td>15 August</td>
</tr>
</tbody>
</table>

              </article>
            </div>
          </div>
        </div>
      </div>

    

  </div>
  <div>&nbsp;</div>
  </div><script>
    function showCanonicalImages() {
      var images = document.getElementsByTagName('img');
      if (!images) {
        return;
      }
      for (var index = 0; index < images.length; index++) {
        var image = images[index];
        if (image.getAttribute('data-canonical-src') && image.src !== image.getAttribute('data-canonical-src')) {
          image.src = image.getAttribute('data-canonical-src');
        }
      }
    }

    function scrollToHash() {
      if (location.hash && !document.querySelector(':target')) {
        var element = document.getElementById('user-content-' + location.hash.slice(1));
        if (element) {
           element.scrollIntoView();
        }
      }
    }

    function autorefreshContent(eventSourceUrl) {
      var initialTitle = document.title;
      var contentElement = document.getElementById('grip-content');
      var source = new EventSource(eventSourceUrl);
      var isRendering = false;

      source.onmessage = function(ev) {
        var msg = JSON.parse(ev.data);
        if (msg.updating) {
          isRendering = true;
          document.title = '(Rendering) ' + document.title;
        } else {
          isRendering = false;
          document.title = initialTitle;
          contentElement.innerHTML = msg.content;
          showCanonicalImages();
        }
      }

      source.onerror = function(e) {
        if (e.readyState === EventSource.CLOSED && isRendering) {
          isRendering = false;
          document.title = initialTitle;
        }
      }
    }

    window.onhashchange = function() {
      scrollToHash();
    }

    window.onload = function() {
      scrollToHash();
    }

    showCanonicalImages();

    var autorefreshUrl = document.getElementById('preview-page').getAttribute('data-autorefresh-url');
    if (autorefreshUrl) {
      autorefreshContent(autorefreshUrl);
    }
  </script>
</body>
</html>